{
  "task": "denoising_color_50(dranet_low_iter)"     //  color Gaussian denoising for noise level 15/25/50. root/task/images-models-options
  , "model": "plain" // "plain" | "plain2" if two inputs
  , "gpu_ids": [1]
  , "dist": true

  , "n_channels": 3  // broadcast to "datasets", 1 for grayscale, 3 for color

  , "path": {
    "root": "denoising_color"            // "denoising" | "superresolution" | "dejpeg"
    , "pretrained_netG": null      // path of pretrained model
    , "pretrained_netE": null      // path of pretrained model
  }

  , "datasets": {
    "train": {
      "name": "train_dataset"           // just name
      , "dataset_type": "dncnn"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "trainsets/trainH/"// path of H training dataset. DIV2K (800 training images) + Flickr2K (2650 images) + BSD500 (400 training&testing images) + WED(4744 images) in SwinIR
      , "dataroot_L": null              // path of L training dataset

      , "H_size": 128                   // patch_size
      , "sigma": 50                     //  15 | 25 | 50. We fine-tune sigma=25/50 models from sigma=15 model, so that `G_optimizer_lr` and `G_scheduler_milestones` can be halved to save time.
      , "sigma_test": 50                // 

      , "dataloader_shuffle": true
      , "dataloader_num_workers": 16
      , "dataloader_batch_size": 8      // batch size 1 | 16 | 32 | 48 | 64 | 128. Total batch size =1x8=8 in SwinIR
    }
    , "test": {
      "name": "test_dataset"            // just name
      , "dataset_type": "dncnn"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "testsets/McMaster"  // path of H testing dataset
      , "dataroot_L": null              // path of L testing dataset

      , "sigma": 50                     // 
      , "sigma_test": 50                // 

    }
  }

  , "netG": {
    "net_type": "dranet"
    , "in_nc": 3        // input channel number
    , "out_nc": 3       // ouput channel number
    , "nc": 128       // 17 for "dncnn", 20 for dncnn3, 16 for "srresnet"
    , "bias": false
    , "init_type": "orthogonal"         // "orthogonal" | "normal" | "uniform" | "xavier_normal" | "xavier_uniform" | "kaiming_normal" | "kaiming_uniform"
    , "init_bn_type": "uniform"         // "uniform" | "constant"
    , "init_gain": 0.2
  }

  , "train": {
    "G_lossfn_type": "l2"      // "l1" | "l2sum" | "l2" | "ssim" | "charbonnier" preferred
    , "G_lossfn_weight": 1.0            // default
    , "G_lossfn_weight1": 0.01            // default
    , "G_lossfn_weight2": 0.001            // default

    // , "hf_loss": true   // hf loss 추가
    
    , "E_decay": 0.999                  // Exponential Moving Average for netG: set 0 to disable; default setting 0.999

    , "G_optimizer_type": "adam"        // fixed, adam is enough
    , "G_optimizer_lr": 1e-4            // learning rate
    // , "G_optimizer_wd": 0               // weight decay, default 0
    , "G_optimizer_clipgrad": null      // unused
    // , "G_optimizer_reuse": true         // 
 
    
    , "G_scheduler_type": "MultiStepLR" // "MultiStepLR" is enough
    , "G_scheduler_milestones": [100000,200000,300000,400000,500000]//[800000, 1200000, 1400000, 1500000, 1600000]
    , "G_scheduler_gamma": 0.5



    , "G_regularizer_orthstep": null    // unused
    , "G_regularizer_clipstep": null    // unused

    , "G_param_strict": true
    , "E_param_strict": true

    , "checkpoint_test": 5000           // for testing
    , "checkpoint_save": 5000           // for saving model
    , "checkpoint_print": 200           // for print
  }
}
